{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplos Prácticos con JAX\n",
    "\n",
    "Este cuaderno demuestra las capacidades principales de JAX mediante ejemplos prácticos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup e Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"Backend: {jax.default_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transformación `grad`: Diferenciación Automática\n",
    "\n",
    "JAX puede calcular derivadas automáticamente de cualquier función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función simple\n",
    "def f(x):\n",
    "    return x**3 + 2*x**2 - 5*x + 3\n",
    "\n",
    "# Calcular derivada\n",
    "df_dx = grad(f)\n",
    "\n",
    "x = 2.0\n",
    "print(f\"f({x}) = {f(x)}\")\n",
    "print(f\"f'({x}) = {df_dx(x)}\")\n",
    "print(f\"Esperado: 3*{x}^2 + 4*{x} - 5 = {3*x**2 + 4*x - 5}\")\n",
    "\n",
    "# Segunda derivada\n",
    "d2f_dx2 = grad(grad(f))\n",
    "print(f\"f''({x}) = {d2f_dx2(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transformación `jit`: Compilación JIT\n",
    "\n",
    "JIT compila funciones con XLA para mayor rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slow_function(x):\n",
    "    for _ in range(10):\n",
    "        x = jnp.dot(x, x)\n",
    "    return x\n",
    "\n",
    "# Versión compilada\n",
    "fast_function = jit(slow_function)\n",
    "\n",
    "x = jnp.ones((100, 100))\n",
    "\n",
    "# Calentar JIT\n",
    "_ = fast_function(x).block_until_ready()\n",
    "\n",
    "# Comparar tiempos\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "_ = slow_function(x).block_until_ready()\n",
    "time_slow = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "_ = fast_function(x).block_until_ready()\n",
    "time_fast = time.time() - start\n",
    "\n",
    "print(f\"Sin JIT: {time_slow*1000:.3f} ms\")\n",
    "print(f\"Con JIT: {time_fast*1000:.3f} ms\")\n",
    "print(f\"Aceleración: {time_slow/time_fast:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transformación `vmap`: Vectorización Automática\n",
    "\n",
    "vmap aplica automáticamente funciones a batches de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_norm(vector):\n",
    "    \"\"\"Calcula la norma L2 de un vector\"\"\"\n",
    "    return jnp.sqrt(jnp.sum(vector ** 2))\n",
    "\n",
    "# Batch de vectores\n",
    "vectors = jnp.array([[1.0, 2.0, 3.0],\n",
    "                     [4.0, 5.0, 6.0],\n",
    "                     [7.0, 8.0, 9.0]])\n",
    "\n",
    "print(\"Vectores:\")\n",
    "print(vectors)\n",
    "\n",
    "# Con vmap (automático)\n",
    "compute_norms_batch = vmap(compute_norm)\n",
    "norms = compute_norms_batch(vectors)\n",
    "\n",
    "print(f\"\\nNormas: {norms}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ejemplo: Regresión Lineal\n",
    "\n",
    "Implementación completa de regresión lineal con gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar datos sintéticos\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "true_w, true_b = 3.5, 2.0\n",
    "\n",
    "X = np.random.uniform(-5, 5, n_samples)\n",
    "y = true_w * X + true_b + np.random.normal(0, 1, n_samples)\n",
    "\n",
    "X_jax = jnp.array(X)\n",
    "y_jax = jnp.array(y)\n",
    "\n",
    "# Visualizar datos\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(X, y, alpha=0.5)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Datos de entrenamiento')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Parámetros verdaderos: w={true_w}, b={true_b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo y función de pérdida\n",
    "def predict(params, X):\n",
    "    return params['w'] * X + params['b']\n",
    "\n",
    "def mse_loss(params, X, y):\n",
    "    predictions = predict(params, X)\n",
    "    return jnp.mean((predictions - y) ** 2)\n",
    "\n",
    "# Inicializar parámetros\n",
    "key = jax.random.PRNGKey(0)\n",
    "params = {\n",
    "    'w': jax.random.normal(key, shape=()),\n",
    "    'b': jax.random.normal(key, shape=())\n",
    "}\n",
    "\n",
    "print(f\"Parámetros iniciales: w={params['w']:.4f}, b={params['b']:.4f}\")\n",
    "\n",
    "# Compilar gradiente\n",
    "grad_loss = jit(grad(mse_loss))\n",
    "\n",
    "# Entrenamiento\n",
    "learning_rate = 0.01\n",
    "n_epochs = 100\n",
    "history = {'loss': [], 'w': [], 'b': []}\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    grads = grad_loss(params, X_jax, y_jax)\n",
    "    params['w'] -= learning_rate * grads['w']\n",
    "    params['b'] -= learning_rate * grads['b']\n",
    "    \n",
    "    loss = mse_loss(params, X_jax, y_jax)\n",
    "    history['loss'].append(float(loss))\n",
    "    history['w'].append(float(params['w']))\n",
    "    history['b'].append(float(params['b']))\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Época {epoch+1:3d} | Loss: {loss:.4f} | w: {params['w']:.4f} | b: {params['b']:.4f}\")\n",
    "\n",
    "print(f\"\\nParámetros finales: w={params['w']:.4f}, b={params['b']:.4f}\")\n",
    "print(f\"Parámetros reales:  w={true_w}, b={true_b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar resultados\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Regresión\n",
    "axes[0].scatter(X, y, alpha=0.5, label='Datos')\n",
    "x_line = jnp.linspace(X.min(), X.max(), 100)\n",
    "y_pred = predict(params, x_line)\n",
    "y_true_line = true_w * x_line + true_b\n",
    "axes[0].plot(x_line, y_pred, 'r-', linewidth=2, label='Predicción')\n",
    "axes[0].plot(x_line, y_true_line, 'g--', linewidth=2, label='Real')\n",
    "axes[0].set_xlabel('X')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('Regresión Lineal')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[1].plot(history['loss'], linewidth=2)\n",
    "axes[1].set_xlabel('Época')\n",
    "axes[1].set_ylabel('MSE Loss')\n",
    "axes[1].set_title('Evolución de la Pérdida')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ejemplo: Red Neuronal Simple\n",
    "\n",
    "Implementación de una red neuronal para clasificación del dataset Iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Cargar datos\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Dividir y normalizar\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# One-hot encoding\n",
    "def one_hot(y, num_classes):\n",
    "    return jnp.eye(num_classes)[y]\n",
    "\n",
    "X_train = jnp.array(X_train)\n",
    "X_test = jnp.array(X_test)\n",
    "y_train_onehot = one_hot(y_train, 3)\n",
    "y_test_onehot = one_hot(y_test, 3)\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "print(f\"Clases: {iris.target_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar red neuronal\n",
    "def init_network(layer_sizes, key):\n",
    "    params = []\n",
    "    keys = jax.random.split(key, len(layer_sizes) - 1)\n",
    "    \n",
    "    for i, (n_in, n_out) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "        w_key, b_key = jax.random.split(keys[i])\n",
    "        scale = jnp.sqrt(2.0 / (n_in + n_out))\n",
    "        W = jax.random.normal(w_key, (n_in, n_out)) * scale\n",
    "        b = jnp.zeros(n_out)\n",
    "        params.append({'W': W, 'b': b})\n",
    "    \n",
    "    return params\n",
    "\n",
    "# Forward pass\n",
    "def forward(params, x):\n",
    "    activation = x\n",
    "    for i in range(len(params) - 1):\n",
    "        z = jnp.dot(activation, params[i]['W']) + params[i]['b']\n",
    "        activation = jnp.maximum(0, z)  # ReLU\n",
    "    \n",
    "    logits = jnp.dot(activation, params[-1]['W']) + params[-1]['b']\n",
    "    exp_x = jnp.exp(logits - jnp.max(logits, axis=-1, keepdims=True))\n",
    "    return exp_x / jnp.sum(exp_x, axis=-1, keepdims=True)  # Softmax\n",
    "\n",
    "# Loss y accuracy\n",
    "def cross_entropy_loss(params, X, y_true):\n",
    "    y_pred = forward(params, X)\n",
    "    y_pred = jnp.clip(y_pred, 1e-10, 1.0)\n",
    "    return -jnp.mean(jnp.sum(y_true * jnp.log(y_pred), axis=1))\n",
    "\n",
    "def accuracy(params, X, y_true):\n",
    "    y_pred = forward(params, X)\n",
    "    predicted_class = jnp.argmax(y_pred, axis=1)\n",
    "    true_class = jnp.argmax(y_true, axis=1)\n",
    "    return jnp.mean(predicted_class == true_class)\n",
    "\n",
    "# Arquitectura: 4 -> 16 -> 8 -> 3\n",
    "layer_sizes = [4, 16, 8, 3]\n",
    "key = jax.random.PRNGKey(42)\n",
    "params = init_network(layer_sizes, key)\n",
    "\n",
    "print(f\"Arquitectura: {' -> '.join(map(str, layer_sizes))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar\n",
    "grad_loss = jit(grad(cross_entropy_loss))\n",
    "loss_fn = jit(cross_entropy_loss)\n",
    "accuracy_fn = jit(accuracy)\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_epochs = 200\n",
    "history = {'train_loss': [], 'test_loss': [], 'train_acc': [], 'test_acc': []}\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    grads = grad_loss(params, X_train, y_train_onehot)\n",
    "    for i in range(len(params)):\n",
    "        params[i]['W'] -= learning_rate * grads[i]['W']\n",
    "        params[i]['b'] -= learning_rate * grads[i]['b']\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0 or epoch == 0:\n",
    "        train_loss = loss_fn(params, X_train, y_train_onehot)\n",
    "        test_loss = loss_fn(params, X_test, y_test_onehot)\n",
    "        train_acc = accuracy_fn(params, X_train, y_train_onehot)\n",
    "        test_acc = accuracy_fn(params, X_test, y_test_onehot)\n",
    "        \n",
    "        history['train_loss'].append(float(train_loss))\n",
    "        history['test_loss'].append(float(test_loss))\n",
    "        history['train_acc'].append(float(train_acc))\n",
    "        history['test_acc'].append(float(test_acc))\n",
    "        \n",
    "        print(f\"Época {epoch+1:3d} | Train Loss: {train_loss:.4f} | \"\n",
    "              f\"Test Loss: {test_loss:.4f} | Train Acc: {train_acc:.3f} | Test Acc: {test_acc:.3f}\")\n",
    "\n",
    "final_acc = accuracy_fn(params, X_test, y_test_onehot)\n",
    "print(f\"\\nAccuracy final: {final_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar resultados\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train', linewidth=2)\n",
    "axes[0].plot(history['test_loss'], label='Test', linewidth=2)\n",
    "axes[0].set_xlabel('Época (x20)')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Evolución de la Pérdida')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history['train_acc'], label='Train', linewidth=2)\n",
    "axes[1].plot(history['test_acc'], label='Test', linewidth=2)\n",
    "axes[1].set_xlabel('Época (x20)')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Evolución del Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim([0, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Matriz de confusión\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = forward(params, X_test)\n",
    "y_pred_class = jnp.argmax(y_pred, axis=1)\n",
    "y_true_class = jnp.argmax(y_test_onehot, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_true_class, y_pred_class)\n",
    "print(\"\\nMatriz de Confusión:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusión\n",
    "\n",
    "JAX ofrece:\n",
    "- **Diferenciación automática** con `grad`\n",
    "- **Alto rendimiento** con `jit`\n",
    "- **Vectorización** con `vmap`\n",
    "- **API familiar** tipo NumPy\n",
    "\n",
    "Es ideal para investigación y proyectos que requieren máximo control y rendimiento."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
